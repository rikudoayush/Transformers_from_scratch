{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Transformers_from_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bOqszoGSsIzf"
      ],
      "authorship_tag": "ABX9TyMALFm8UWJQguW2slmSxPBK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikudoayush/Transformers_from_scratch/blob/main/Pytorch_Transformers_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPDJYvidqJsy"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOqszoGSsIzf"
      },
      "source": [
        "# Basic Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AugDHEEeq9fW"
      },
      "source": [
        "# assume we have some tensor x with size (b, t, k)\n",
        "##x = \n",
        "\n",
        "#The set of all raw dot products w′ij forms a matrix, which we can compute simply by multiplying 𝐗 by its transpose:\n",
        "##raw_weights = torch.bmm(x,x.transpose(1,2))\n",
        "# - torch.bmm is a batched matrix multiplication. It \n",
        "#   applies matrix multiplication over batches of \n",
        "#   matrices.\n",
        "\n",
        "\n",
        "# Then, to turn the raw weights w′ij into positive values that sum to one, we apply a row-wise softmax:\n",
        "\n",
        "##weights = F.softmax(raw_weights, dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYmHtOjlrrK_"
      },
      "source": [
        "# Finally, to compute the output sequence, we just multiply the weight matrix by 𝐗. This results in a batch of output matrices 𝐘 of size (b, t, k) \n",
        "# whose rows are weighted sums over the rows of 𝐗.\n",
        "##y = torch.bmm(weights,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08VSzkoLS4cp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h0vhh6OsRUn"
      },
      "source": [
        "# Transformer Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLmCfHIQtGJo"
      },
      "source": [
        "\\begin{align*}\n",
        "q_{i} &= W_qx_{i} &\n",
        "k_{i} &= W_kx_{i} &\n",
        "v_{i} &= W_vx_{i} \n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "w'_{{i}{j}} &= q_{i}^Tk_{j} \\\\\n",
        "w_{{i}{j}} &= softmax(w'_{{i}{j}}) \\\\\n",
        "y_{i} &= \\sum_{j} w_{{i}{j}}v_{i}\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "w'_{{i}{j}} = \\frac{{q_{i}}^Tk_{j}}{\\sqrt{k}}\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbokc9YV1won"
      },
      "source": [
        "Every input vector x_i is used in three different ways in the self attention operation:\n",
        "\n",
        "1) It is compared to every other vector to establish the weights for its own output 𝐲_i\n",
        "\n",
        "2) It is compared to every other vector to establish the weights for the output of the j-th vector 𝐲_j\n",
        "\n",
        "3) It is used as part of the weighted sum to compute each output vector once the weights have been established."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CUm-fOsPek"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, k, heads = 8):\n",
        "    super().__init__()\n",
        "    self.k, self.heads = k, heads\n",
        "    # These compute the queries, keys and values for all \n",
        "    # heads (as a single concatenated vector)\n",
        "    self.tokeys = nn.Linear(k, k*heads bias = False)\n",
        "    self.toqueries = nn.Linear(k, k*heads bias = False)\n",
        "    self.tovalues = nn.Linear(k, k*heads bias = False)\n",
        "    # This unifies the outputs of the different heads into \n",
        "\t  # a single k-vector\n",
        "    self.unifyheads = nn.Linear(heads*k, k)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    b, t, k = x.size()\n",
        "    h = self.heads\n",
        "\n",
        "    queries = self.toqueries(x).view(b, t, h, k)\n",
        "    key = self.tokeys(x).view(b, t, h, k)\n",
        "    values = self.tovalues(x).view(b, t, h, k)\n",
        "    # - fold heads into the batch dimension\n",
        "    keys = keys.transpose(1,2).contiguous().view(b*h, t, k)\n",
        "    queries = queries.transpose(1,2).contiguous().view(b*h, t, k)\n",
        "    values = values.transpose(1,2).contiguous().view(b*h, t, k)\n",
        "\n",
        "    queries = queries/(k**(1/4))\n",
        "    keys = keys/(k**(1/4))\n",
        "    # - get dot product of queries and keys, and scale\n",
        "    dot = torch.bmm(queries, key.transpose(1,2))\n",
        "    # - dot has size (b*h, t, t) containing raw weights\n",
        "'''\n",
        "   For text generation :  to ensure that elements can only attend to input elements that precede them in the sequence.\n",
        "                           we’ll need to ensure that it cannot look forward into the sequence. We do this by applying a mask \n",
        "                           to the matrix of dot products, before the softmax is applied. This mask disables all elements above the diagonal of the matrix.\n",
        "\n",
        "    dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "\n",
        "    indices = torch.triu_indices(t, t, offset=1)\n",
        "    dot[:, indices[0], indices[1]] = float('-inf')\n",
        "\n",
        "    dot = F.softmax(dot, dim=2)                       \n",
        "'''\n",
        "\n",
        "    dot = F.softmax(dot, dim=2)\n",
        "    # - dot now contains row-wise normalized weights\n",
        "    out = torch.bmm(out, values).view(b, h, t, k)\n",
        "    # swap h, t back, unify heads\n",
        "    # so that the head dimension and the embedding dimension are next to each other\n",
        "    # We then pass these through the unifyheads layer to project them back down to k dimensions.\n",
        "    out = out.transpose(1,2)contiguous().view(b, t, h * k)\n",
        "    return self.unifyheads(out)\n",
        "  \n",
        "  def forward_einsum(self, x):\n",
        "    b, t, e = x.size()\n",
        "    h = self.heads\n",
        "\n",
        "    keys    = self.tokeys(x).view(b, t, h, e)\n",
        "    queries = self.toqueries(x).view(b, t, h, e)\n",
        "    values  = self.tovalues(x).view(b, t, h, e)\n",
        "\n",
        "    dot = torch.einsum('bthe,bihe->bhti', queries, keys) / math.sqrt(e)\n",
        "    dot = F.softmax(dot, dim=-1)\n",
        "\n",
        "    out = torch.einsum('bhtd,bdhe->bthe', dot, values)\n",
        "\n",
        "    # we can move reshape of weights to init; I left it here just to compare with the original implementation\n",
        "    out = torch.einsum('bthe,khe->btk', out, self.unifyheads.weight.view(e,h,e)) \n",
        "    return out + self.unifyheads.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9btd-9bw8KiD"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  # the block applies, in sequence: a self attention layer, layer normalization, \n",
        "   # a feed forward layer (a single MLP applied independently to each vector), and another layer normalization.\n",
        "\n",
        "  def __init__(self, k, heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = SelfAttention(k, heads=heads)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(k)\n",
        "    self.norm2 = nn.LayerNorm(k)\n",
        "\n",
        "    self.ff = nn.Sequential(nn.Linear(k, 4*k),\n",
        "                            nn.Relu(),\n",
        "                            nn.Linear(4*k, k))\n",
        "  def forward(self, x):\n",
        "    \n",
        "    attended = self.attention(x)\n",
        "    x = self.norm1(attended(x))\n",
        "\n",
        "    fedforward = self.ff(x)\n",
        "\n",
        "    return self.norm2(fedforward+x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_PzWOAeWWiu"
      },
      "source": [
        "# Data Reading and Pre-Processing    for REFERENCE visit (https://github.com/pbloem/former/blob/master/experiments/classify.py)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiDPo6U0WWay"
      },
      "source": [
        "# Data Reading and Pre-Processing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4qKlYjVWWSs"
      },
      "source": [
        "# Data Reading and Pre-Processing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUiXQ0y4WWIZ"
      },
      "source": [
        "# PRE-TRAINED EMBEDDINGS\n",
        "vocab_size = vocab_size\n",
        "embeddings_matrix = np.zeros((vocab_size, emb_dim))\n",
        "model = FastText.load_fasttext_format('C:/Users/Admin/Downloads/FILES/Model/cc.en.300.bin/cc.en.100.bin')\n",
        "for key, value in t_1.word_index.items():\n",
        "    embedding_matrix[value] = model.wv[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOy7vluOS-CO"
      },
      "source": [
        "# CUSTOM EMBEDDINGS\n",
        "'''\n",
        "embeddings_index = dict()\n",
        "f = open('C:/Users/Admin/Downloads/FILES/Model/Script/final1.txt',encoding='utf8')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t_1.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "'''    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anjXLzPPTECZ"
      },
      "source": [
        "'''\n",
        "def create_emb_layer(embedding_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = embedding_matrix.size()\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': embedding_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim\n",
        "'''    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrF4e7C-Ijje"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
        "    super().__init__()\n",
        "    self.num_embeddings, self.embedding_dim = embedding_matrix.size()\n",
        "    self.num_tokens = num_tokens\n",
        "    #self.token_emb = nn.Embeddings(num_tokens, k)\n",
        "    self.token_emb = nn.Embeddings(self.num_embeddings, self.embedding_dim, requires_grad=True)\n",
        "    self.token_emb.load_state_dict({'weight': embedding_matrix})\n",
        "    #self.pos_emb = nn.Embeddings(seq_length, k)\n",
        "    self.pos_emb = nn.Embeddings(seq_length, self.embedding_dim)\n",
        "    # The sequence of transformer blocks that does all the \n",
        "\t\t# heavy lifting\n",
        "    tblocks = []\n",
        "    for i in range(depth):\n",
        "      tblocks.append(TransformerBlock(k = k, heads = heads))\n",
        "    self.tblocks = nn.Sequential(*tblocks)\n",
        "    # Maps the final output sequence to class logits \n",
        "    #self.toprobs = nn.Linear(k, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "  \"\"\"\n",
        "        :param x: A (b, t) tensor of integer values representing \n",
        "                  words (in some predetermined vocabulary).\n",
        "        :return: A (b, c) tensor of log-probabilities over the \n",
        "                 classes (where c is the nr. of classes).\n",
        "  \"\"\"\n",
        "     # generate token embeddings\n",
        "     tokens = self.token_emb(x)\n",
        "     b, t, k = tokens.size()\n",
        "     \n",
        "     # generate position embeddings\n",
        "     positions = torch.arange(t)\n",
        "     positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
        "\n",
        "     x = tokens + postions\n",
        "     x = self.tblocks(x)\n",
        "      \n",
        "     # Average-pool over the t dimension and project to class \n",
        "     # probabilities\n",
        "\n",
        "     #x = self.toprobs(x.mean(dims=1))\n",
        "     #return F.log_softmax(x, dims=1) \n",
        "     return F.sigmoid(x, dims=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiuqhqfCRHll"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IZSRWVURbOS"
      },
      "source": [
        "lr = 0.0001\n",
        "lr_warmup = 10000\n",
        "num_heads = 8\n",
        "depth = 6\n",
        "batch_size = 16\n",
        "num_epochs = 10\n",
        "gradient_clipping = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG2ovOmsRH90"
      },
      "source": [
        "model = former.CTransformer(emb=embedding_size, heads=num_heads, depth=depth, seq_length=mx, num_tokens=vocab_size, num_classes=NUM_CLS, max_pool=max_pool)\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "\n",
        "opt = torch.optim.Adam(lr=lr, params=model.parameters())\n",
        "sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (lr_warmup / batch_size), 1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ--juErSOFK"
      },
      "source": [
        "def train():\n",
        "    seen = 0\n",
        "    for e in range(num_epochs):\n",
        "\n",
        "        print(f'\\n epoch {e}')\n",
        "        model.train(True)\n",
        "\n",
        "        for batch in tqdm.tqdm(train_iter):\n",
        "\n",
        "            opt.zero_grad()\n",
        "\n",
        "            input = batch.text[0]\n",
        "            label = batch.label - 1\n",
        "\n",
        "            if input.size(1) > mx:\n",
        "                input = input[:, :mx]\n",
        "            out = model(input)\n",
        "            loss = F.nll_loss(out, label)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # clip gradients\n",
        "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
        "            if gradient_clipping > 0.0:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
        "\n",
        "            opt.step()\n",
        "            sch.step()\n",
        "\n",
        "            seen += input.size(0)\n",
        "            tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            model.train(False)\n",
        "            tot, cor= 0.0, 0.0\n",
        "\n",
        "            for batch in test_iter:\n",
        "\n",
        "                input = batch.text[0]\n",
        "                label = batch.label - 1\n",
        "\n",
        "                if input.size(1) > mx:\n",
        "                    input = input[:, :mx]\n",
        "                out = model(input).argmax(dim=1)\n",
        "\n",
        "                tot += float(input.size(0))\n",
        "                cor += float((label == out).sum().item())\n",
        "\n",
        "            acc = cor / tot\n",
        "            print(f'-- {\"test\" if arg.final else \"validation\"} accuracy {acc:.3}')\n",
        "            tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n",
        "\n",
        "\n",
        "train()            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTdv_dObhp0f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}